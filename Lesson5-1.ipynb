{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction to Python\n",
    "================================\n",
    "\n",
    "Lesson 5 \n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we will explore a little more in dept some aspects of Keras and neural networks.\n",
    "The topis that we'll cover in this lesson are:\n",
    "\n",
    "  - Dropout\n",
    "  - Grid Search\n",
    "  - Autoencoders\n",
    "  \n",
    "But before starting with the real Lesson let's make a little bit of informations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Informations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exam modality \n",
    "\n",
    "The exams will be a code snippet to comment.\n",
    "\n",
    "It may be required to give an high level or a *line by line* explanation.\n",
    "\n",
    "The arguments will be the same that we have touched in this four lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the lesson\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Droptout\n",
    "\n",
    "Dropout is a method often used on neural networks in order to avoid/reduce orverfitting.\n",
    "\n",
    "The idea is to randomly shut down some neurons at each iteration in order to make them less relevant for the network.   \n",
    "\n",
    "For this dropout is a layer itself, as you can see from the [Keras docs](https://keras.io/layers/core/), that we put on top of another layer.\n",
    "\n",
    "Let's see how we can improve the network that we have created for the **mnist** classification task last lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-factoring\n",
    "\n",
    "In order to improve the code even more, we'll make a little bit of re-factoring and re-write it using Python Classes.\n",
    "\n",
    "**NOTE**: please note how the names of classes and function are written. As you can see \n",
    "\n",
    "  - every class has a name that starts with a **uppercase** letter\n",
    "  - every function has a name that starts with a **lowercase** letter\n",
    "  \n",
    "This is due to a style convention used in Python coding called [pep8](https://www.python.org/dev/peps/pep-0008/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give an *high level* description of what the code does\n",
    "  - Give a *line by line* description of the methods `modelDefinition` and the class `PlotGraphs`.\n",
    "\n",
    "**HINT**: `string.find(substring)` searches for a substring in a string. Returns `True` if present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "metrics = ['accuracy','mae']\n",
    "def testBinary(y_true,y_pred):\n",
    "    keras.metrics.binary_accuracy(y_true,y_pred,threshold=0.5)\n",
    "    \n",
    "\n",
    "\n",
    "class PrepareVariabiles:\n",
    "    def __init__(self):\n",
    "        self.mnist = keras.datasets.mnist\n",
    "        #self.mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "        \n",
    "        \n",
    "    def splitDatset(self):\n",
    "        (X, Y), (X_test, Y_test) = self.mnist.load_data()\n",
    "        X, X_test = X / 255.0, X_test / 255.0\n",
    "        return X,Y,X_test,Y_test\n",
    "    \n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.activationFun = 'relu'\n",
    "        self.dropPerc = 0.25\n",
    "        self.X = kargs['x']\n",
    "        self.Y = kargs['y']\n",
    "        self.X_test = kargs['xt']\n",
    "        self.Y_test = kargs['yt']\n",
    "        self.log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        \n",
    "    def modelDefinition(self,useDropout=False):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Flatten(input_shape=(28, 28)))\n",
    "        self.model.add(Dense(num_input,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_1,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_2,activation=self.activationFun))\n",
    "        self.model.add(Dense(num_classes,activation='softmax'))\n",
    "        \n",
    "    def modelCompile(self):\n",
    "        adam = Adam(lr=learning_rate)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=adam,metrics=['accuracy'])\n",
    "        \n",
    "    def modelEval(self):\n",
    "        tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=self.log_dir, histogram_freq=1,write_graph=True, write_images=True,embeddings_freq=0) \n",
    "        history = self.model.fit(self.X, self.Y\n",
    "                                 ,epochs=num_steps\n",
    "                                 ,batch_size=batch_size\n",
    "                                 ,validation_split=0.05\n",
    "                                 ,callbacks=[tbCallBack])#validation_data=(self.X_test,self.Y_test)\n",
    "        scores = self.model.evaluate(self.X_test, self.Y_test)\n",
    "        return history,scores,self.model\n",
    "    \n",
    "class PlotGraphs:\n",
    "    def __init__(self,**kargs):\n",
    "        history= kargs['h']\n",
    "        self.history_dict = history.history\n",
    "        self.metList = []\n",
    "        for cur_key in history.history.keys():\n",
    "            if cur_key.find('val')!=0:\n",
    "                self.metList.append(cur_key)\n",
    "        print(self.metList)\n",
    "        self.lenList = len(self.metList)\n",
    "    \n",
    "    def plotResults(self):\n",
    "        plotPos = 1\n",
    "        for cur_met in self.metList:\n",
    "            cur_values = self.history_dict[cur_met]\n",
    "            cur_val = self.history_dict['val_%s'%cur_met]\n",
    "            epochs = range(1, len(cur_values) + 1)\n",
    "            plt.subplot(self.lenList, 1, plotPos)\n",
    "            plt.plot(epochs, cur_values, 'ro')\n",
    "            plt.plot(epochs, cur_val, 'b+')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(cur_met)\n",
    "            plotPos += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = datetime.now()\n",
    "    logger.info('START')\n",
    "    logger.info('INITIALIZATION OF PREPAREVARIABILES')\n",
    "    pv = PrepareVariabiles()\n",
    "    X, Y, X_test, Y_test = pv.splitDatset()\n",
    "    logger.info('INITIALIZATION OF CREATENN')\n",
    "    cnn = CreateNN(x=X,y=Y,xt=X_test,yt=Y_test)\n",
    "    cnn.modelDefinition()#useDropout=True\n",
    "    cnn.modelCompile()\n",
    "    history, scores, model = cnn.modelEval()\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    logger.info('INITIALIZATION OF PLOTGRAPH')\n",
    "    pg = PlotGraphs(h=history)\n",
    "    pg.plotResults()\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info('END')\n",
    "    return history,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:04:43,119 : INFO : START\n",
      "2020-11-01 16:04:43,121 : INFO : INITIALIZATION OF PREPAREVARIABILES\n",
      "2020-11-01 16:04:44,463 : INFO : INITIALIZATION OF CREATENN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  1/446 [..............................] - ETA: 0s - loss: 2.3456 - accuracy: 0.0625WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:04:45,986 : WARNING : From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 11s 24ms/step - loss: 0.2196 - accuracy: 0.9350 - val_loss: 0.0824 - val_accuracy: 0.9790\n",
      "Epoch 2/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0846 - accuracy: 0.9727 - val_loss: 0.0771 - val_accuracy: 0.9797\n",
      "Epoch 3/15\n",
      "446/446 [==============================] - 9s 21ms/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0623 - val_accuracy: 0.9843\n",
      "Epoch 4/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0404 - accuracy: 0.9869 - val_loss: 0.0560 - val_accuracy: 0.9850\n",
      "Epoch 5/15\n",
      "446/446 [==============================] - 9s 20ms/step - loss: 0.0293 - accuracy: 0.9902 - val_loss: 0.0570 - val_accuracy: 0.9857\n",
      "Epoch 6/15\n",
      "446/446 [==============================] - 10s 21ms/step - loss: 0.0260 - accuracy: 0.9915 - val_loss: 0.0779 - val_accuracy: 0.9800\n",
      "Epoch 7/15\n",
      "446/446 [==============================] - 9s 21ms/step - loss: 0.0226 - accuracy: 0.9926 - val_loss: 0.0862 - val_accuracy: 0.9800\n",
      "Epoch 8/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.0711 - val_accuracy: 0.9827\n",
      "Epoch 9/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0830 - val_accuracy: 0.9803\n",
      "Epoch 10/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0173 - accuracy: 0.9945 - val_loss: 0.0788 - val_accuracy: 0.9820\n",
      "Epoch 11/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0842 - val_accuracy: 0.9830\n",
      "Epoch 12/15\n",
      "446/446 [==============================] - 9s 20ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0655 - val_accuracy: 0.9860\n",
      "Epoch 13/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0858 - val_accuracy: 0.9813\n",
      "Epoch 14/15\n",
      "446/446 [==============================] - 9s 21ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.1120 - val_accuracy: 0.9760\n",
      "Epoch 15/15\n",
      "446/446 [==============================] - 10s 22ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0916 - val_accuracy: 0.9830\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.0892 - accuracy: 0.9817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:07:11,880 : INFO : INITIALIZATION OF PLOTGRAPH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 98.17%\n",
      "['loss', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:07:11,951 : INFO : EXECUTED IN 148.831687 SEC\n",
      "2020-11-01 16:07:11,955 : INFO : END\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "# for cur_key in history.history.keys():\n",
    "#     if cur_key.find('val')!=0:\n",
    "#         print(cur_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Tensorboard is a tool for make a visual inspection on a network created with Tensorflow (ans so also with Keras).\n",
    "\n",
    "The argument is very complex and you can find more information at [Keras callbacks man page](https://keras.io/callbacks/). We won't go in any details now.\n",
    "\n",
    "The command that enables us to use Tendorboard is\n",
    "\n",
    "`tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph',write_graph=True, write_images=True,embeddings_freq=0)`\n",
    "\n",
    "The callback is used in fit by `callbacks=[tbCallBack]` ad produces a series of file in the `log_dir` folder (it also create it).\n",
    "\n",
    "The result is visible running `tensorboard --logdir Graph` and opening at `http://localhost:6006` in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 24953), started 0:00:57 ago. (Use '!kill 24953' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d162de96d63eb60a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d162de96d63eb60a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "In order to make highlight some implementation's difference and make the code more general, we'll see how to use the grid search over two different kind of models:\n",
    "\n",
    "  - NN (`keras`)\n",
    "  - Random Forest (`sklearn`)\n",
    "  \n",
    "**NOTE**: Since all `sklearn` models have the same structure, the same code can be used for other algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search \n",
    "\n",
    "The `grid search` is the names the `sklearn` uses to indicate the **hyper parameter optimization** ([form wiki](https://en.wikipedia.org/wiki/Hyperparameter_optimization)):\n",
    "\n",
    "*In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.*\n",
    "\n",
    "The package that we'll use is `GridSearchCV` from `sklearn.model_selection` ([documentation page](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "As you can see at main Sklearn's page about [Tuning Hyper Parameters](http://scikit-learn.org/stable/modules/grid_search.html), this is just one of the possibility that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give a *line by line* description of the `LoadData` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#general\n",
    "import os, traceback\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#SKLearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "DEF_MODEL_NAME='best_model_grid'\n",
    "CAT_LIST=['categ']\n",
    "#Prameters for RFC\n",
    "param_grid_RFC = dict(n_estimators=[20,50],max_features=['auto',2],max_depth=[5,10,None],min_samples_leaf=[1,10])\n",
    "#Parameters for NN\n",
    "batch_size = [5,10]\n",
    "epochs = [5,10]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation = ['softmax','relu']\n",
    "optimizer = ['SGD']#, 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid_NN = dict(batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "# param_grid_NN = dict(activation=['relu','tanh'])\n",
    "PARAM_DICT={'RFC':param_grid_RFC,'NN':param_grid_NN}\n",
    "#Files\n",
    "trainFile = 'promoted.csv'\n",
    "predFile = 'target.csv'\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.path = 'data'\n",
    "        self.trainFile = kargs['tr']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = os.path.join(self.path,fileName)\n",
    "        logger.info('READING %s',fullPath)\n",
    "        df = pd.read_csv(fullPath,sep=',',dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',})#dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',}\n",
    "        logger.info('LOADED DATASET WITH SHAPE %s AND COLUMUNS %s',str(df.shape),str(df.columns))\n",
    "        print('After reading',df.describe())\n",
    "        return df\n",
    "        \n",
    "    def prepareTrain(self):\n",
    "        dfTrain = self.readFiles(self.trainFile)\n",
    "        dfTrain.dropna(inplace=True)\n",
    "        X_train = dfTrain.drop(columns=['resp','customer_id'])\n",
    "        Y_train = dfTrain.loc[:,'resp']\n",
    "        mmscaler = preprocessing.MinMaxScaler()\n",
    "        X_train[['card_tenure', 'risk_score', 'num_promoted']] = mmscaler.fit_transform(X_train[['card_tenure', 'risk_score', 'num_promoted']])  \n",
    "        for curCol in ['avg_bal','geo_group', 'res_type']:\n",
    "            X_train[curCol] = dfTrain[curCol].cat.codes\n",
    "        logger.info('AFTER PREPROCESSING X_train HAS COLUMUNS %s AND TYPES %s',str(X_train.columns),str(X_train.dtypes))\n",
    "        return X_train, Y_train\n",
    "\n",
    "\n",
    "class TestClass:\n",
    "\n",
    "    def __init__(self, **kargs):\n",
    "        # Network Parameters\n",
    "        self.n_hidden_1 = 25 \n",
    "        #self.n_hidden_2 = 10 \n",
    "        self.num_input = 6 \n",
    "        \n",
    "        self.num_classes = 1 \n",
    "        self.activationFun = 'relu'\n",
    "\n",
    "    def createModelNN(self,optimizer):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.num_input, input_dim=self.num_input))\n",
    "        model.add(Dense(self.n_hidden_1 ))\n",
    "        model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        # sgd = SGD(lr=model_param['lr'], momentum=model_param['momentum'], decay=0.0, nesterov=False)\n",
    "        #sgd=\"SGD\"\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def createModelRFC(self):\n",
    "        logger.info(\"DEFINITION OF THE MODEL RFC\")\n",
    "        model = RandomForestClassifier(n_jobs=-1)\n",
    "        logger.info(\"MODEL PARAMS: %s\",model.get_params(deep=True))\n",
    "        return model\n",
    "\n",
    "\n",
    "def gridSearch(model,param_grid,X,Y,scoring):\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,scoring=scoring,refit='RMS',verbose=3) \n",
    "    grid_result = grid.fit(X,Y)\n",
    "    logger.info(\"END GRID SEARCH\")\n",
    "    return grid_result\n",
    "\n",
    "def gridResults(grid_result,X,nameModel):\n",
    "    logger.info(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    print(sorted(grid_result.cv_results_.keys()))\n",
    "    if nameModel == \"RFC\":\n",
    "        importances = grid_result.best_estimator_.feature_importances_\n",
    "    if nameModel != 'NN':\n",
    "        df_imp = pd.DataFrame({'features': X.columns.tolist(),'importances':importances})\n",
    "        df_imp.sort_values(by=\"importances\",ascending=False,inplace=True)\n",
    "        print(df_imp)\n",
    "\n",
    "# def SaveModel(nameModel,grid_result):\n",
    "#     if nameModel=='NN':\n",
    "#         outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"h5\")\n",
    "#         grid_result.best_estimator_.save(outFile) \n",
    "        \n",
    "#     else:\n",
    "#         outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"pkl\")\n",
    "#         joblib.dump(grid_result.best_estimator_,outFile)\n",
    "#     logger.info(\"SAVED BEST MODEL IN %s\",outFile)\n",
    "\n",
    "def main(nameModel):\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"START\")\n",
    "    logger.info('READING THE HYPER PARAMETERS FOR THE SELECTED MODEL')\n",
    "    param_grid = PARAM_DICT[nameModel]\n",
    "    logger.info('DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH')\n",
    "    scoring = {'RMS':make_scorer(r2_score)}\n",
    "    # scoring = {'Accuracy': make_scorer(accuracy_score),'RMS':make_scorer(mean_squared_error)}\n",
    "    logger.info(\"LOADING THE DATA SET\")\n",
    "    ld = LoadData(tr=trainFile)\n",
    "    df2Pred = ld.readFiles(predFile)\n",
    "    X, Y = ld.prepareTrain()\n",
    "    logger.info(\"CREATION OF THE MODEL\")\n",
    "    t=TestClass()\n",
    "    if nameModel == 'NN':\n",
    "        model = KerasClassifier(build_fn=t.createModelNN)\n",
    "    else:\n",
    "        model = t.createModelRFC()\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid_result = gridSearch(model,param_grid,X,Y,scoring)\n",
    "    logger.info(\"END OF GRID SEARCH\")\n",
    "    logger.info(\"PRINTING RESULTS\")\n",
    "    gridResults(grid_result,X,nameModel)\n",
    "    #SaveModel(nameModel,grid_result)\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info(\"END\")\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:37:15,884 : INFO : START\n",
      "2020-11-01 16:37:15,885 : INFO : READING THE HYPER PARAMETERS FOR THE SELECTED MODEL\n",
      "2020-11-01 16:37:15,886 : INFO : DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH\n",
      "2020-11-01 16:37:15,888 : INFO : LOADING THE DATA SET\n",
      "2020-11-01 16:37:15,888 : INFO : READING data/target.csv\n",
      "2020-11-01 16:37:16,027 : INFO : LOADED DATASET WITH SHAPE (110000, 8) AND COLUMUNS Index(['customer_id', 'card_tenure', 'risk_score', 'num_promoted', 'avg_bal',\n",
      "       'geo_group', 'res_type', 'Unnamed: 7'],\n",
      "      dtype='object')\n",
      "2020-11-01 16:37:16,065 : INFO : READING data/promoted.csv\n",
      "2020-11-01 16:37:16,098 : INFO : LOADED DATASET WITH SHAPE (25000, 8) AND COLUMUNS Index(['customer_id', 'resp', 'card_tenure', 'risk_score', 'num_promoted',\n",
      "       'avg_bal', 'geo_group', 'res_type'],\n",
      "      dtype='object')\n",
      "2020-11-01 16:37:16,152 : INFO : AFTER PREPROCESSING X_train HAS COLUMUNS Index(['card_tenure', 'risk_score', 'num_promoted', 'avg_bal', 'geo_group',\n",
      "       'res_type'],\n",
      "      dtype='object') AND TYPES card_tenure     float64\n",
      "risk_score      float64\n",
      "num_promoted    float64\n",
      "avg_bal           int16\n",
      "geo_group          int8\n",
      "res_type           int8\n",
      "dtype: object\n",
      "2020-11-01 16:37:16,154 : INFO : CREATION OF THE MODEL\n",
      "2020-11-01 16:37:16,155 : INFO : START GRID SEARCH\n",
      "2020-11-01 16:37:16,156 : INFO : START GRID SEARCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading          card_tenure     risk_score   num_promoted  Unnamed: 7\n",
      "count  107792.000000  110000.000000  110000.000000         0.0\n",
      "mean      138.956564     655.571482       0.006782         NaN\n",
      "std        67.433081      81.252328       0.082183         NaN\n",
      "min        12.000000     520.000000       0.000000         NaN\n",
      "25%        91.000000     600.000000       0.000000         NaN\n",
      "50%       135.000000     678.000000       0.000000         NaN\n",
      "75%       179.000000     720.000000       0.000000         NaN\n",
      "max       641.000000     760.000000       2.000000         NaN\n",
      "After reading                resp   card_tenure    risk_score  num_promoted\n",
      "count  25000.000000  24515.000000  25000.000000  25000.000000\n",
      "mean       0.068640    139.491617    655.091680      0.007000\n",
      "std        0.252846     66.998010     81.315116      0.083374\n",
      "min        0.000000      0.000000    520.000000      0.000000\n",
      "25%        0.000000     95.000000    599.000000      0.000000\n",
      "50%        0.000000    135.000000    677.000000      0.000000\n",
      "75%        0.000000    179.000000    719.000000      0.000000\n",
      "max        1.000000    641.000000    760.000000      1.000000\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  4.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4480/4480 [==============================] - 18s 4ms/step - loss: 5.3668 - accuracy: 0.8774\n",
      "Epoch 2/10\n",
      "4480/4480 [==============================] - 15s 3ms/step - loss: 0.8591 - accuracy: 0.8934\n",
      "Epoch 3/10\n",
      "4480/4480 [==============================] - 17s 4ms/step - loss: 0.5902 - accuracy: 0.9023\n",
      "Epoch 4/10\n",
      "4480/4480 [==============================] - 17s 4ms/step - loss: 0.5000 - accuracy: 0.9072\n",
      "Epoch 5/10\n",
      "4480/4480 [==============================] - 16s 4ms/step - loss: 0.4026 - accuracy: 0.9142\n",
      "Epoch 6/10\n",
      "4480/4480 [==============================] - 16s 4ms/step - loss: 0.3550 - accuracy: 0.9222\n",
      "Epoch 7/10\n",
      "4480/4480 [==============================] - 15s 3ms/step - loss: 0.2710 - accuracy: 0.9301\n",
      "Epoch 8/10\n",
      "4480/4480 [==============================] - 16s 3ms/step - loss: 0.2534 - accuracy: 0.9314\n",
      "Epoch 9/10\n",
      "4480/4480 [==============================] - 16s 3ms/step - loss: 0.2554 - accuracy: 0.9314\n",
      "Epoch 10/10\n",
      "4480/4480 [==============================] - 17s 4ms/step - loss: 0.2537 - accuracy: 0.9313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 16:44:11,689 : INFO : END GRID SEARCH\n",
      "2020-11-01 16:44:11,693 : INFO : END OF GRID SEARCH\n",
      "2020-11-01 16:44:11,694 : INFO : PRINTING RESULTS\n",
      "2020-11-01 16:44:11,695 : INFO : Best: -0.073640 using {'batch_size': 5, 'epochs': 10, 'optimizer': 'SGD'}\n",
      "2020-11-01 16:44:11,696 : INFO : EXECUTED IN 415.812017 SEC\n",
      "2020-11-01 16:44:11,697 : INFO : END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_fit_time', 'mean_score_time', 'mean_test_RMS', 'param_batch_size', 'param_epochs', 'param_optimizer', 'params', 'rank_test_RMS', 'split0_test_RMS', 'split1_test_RMS', 'split2_test_RMS', 'split3_test_RMS', 'split4_test_RMS', 'std_fit_time', 'std_score_time', 'std_test_RMS']\n"
     ]
    }
   ],
   "source": [
    "grid_result = main('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 5, 'epochs': 10, 'optimizer': 'SGD'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid_result.\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning Exercise  \n",
    "\n",
    "On the the code chunk above there are a couple a consideration to make:\n",
    "\n",
    "  1. **Parallelization**: note how the parallelization it's handled. Is there something to consider (hint: `n_jobs`) \n",
    "  2. **Parameters**: for the **NN** take note of what are the parameters of the model and what is the results. Is there something to notice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "Autoencoders are the half way between *supervised* and *unsupervised* learning: the input and the output are the same, so the networking it's learning from itself.\n",
    "\n",
    "For this reason they are called **self-supervised**.\n",
    "\n",
    "From the [man page](https://blog.keras.io/building-autoencoders-in-keras.html) of Keras:\n",
    "\n",
    "*Autoencoding is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.*\n",
    "\n",
    "  1. *Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.*\n",
    "  2. *Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.*\n",
    "  3. *Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.*\n",
    "  \n",
    "What autoencoders are good for:\n",
    "\n",
    "  - Data Denoising\n",
    "  - Dimension Reduction\n",
    "  - Data Visualization (basically the same as 2, but plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "encoding_dim = 32\n",
    "num_images = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.input_dim = kargs['id']\n",
    "        self.compression_factor = float(self.input_dim) / encoding_dim\n",
    "        logger.debug(\"COMPRESSING FACTOR: %s\",str(self.compression_factor))\n",
    "        self.actFun = 'relu'\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 256\n",
    "        self.x_train = kargs['xt']\n",
    "        self.x_test =kargs['xte']\n",
    "        \n",
    "    def createAutoEncoder(self):\n",
    "        self.autoencoder = Sequential()\n",
    "        self.autoencoder.add(Dense(encoding_dim, input_shape=(self.input_dim,), activation=self.actFun))\n",
    "        self.autoencoder.add(Dense(self.input_dim, activation='sigmoid'))\n",
    "        logger.info('SUMMARY OF THE AUTOENCODER MODEL')\n",
    "        print(self.autoencoder.summary())\n",
    "        \n",
    "        \n",
    "    def layerEncoder(self):\n",
    "        #Cration of a tensor\n",
    "        input_img = Input(shape=(self.input_dim,))\n",
    "        logger.info('LAYERS OF THE MODEL')\n",
    "        print(self.autoencoder.layers)\n",
    "        logger.info('SELECTING THE FIRST LAYER')\n",
    "        encoder_layer = self.autoencoder.layers[0]\n",
    "        self.encoder = Model(input_img, encoder_layer(input_img))#input and output \n",
    "        logger.info('SUMMARY OF THE ENCODER MODEL')\n",
    "        print(self.encoder.summary())\n",
    "    \n",
    "    def modelFit(self):\n",
    "        self.autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        self.autoencoder.fit(self.x_train, self.x_train,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                validation_split=0.10)#validation_data=(self.x_test, self.x_test)\n",
    "        return self.autoencoder\n",
    "    \n",
    "    def modelPredict(self):\n",
    "        encoded_imgs = self.encoder.predict(self.x_test)\n",
    "        decoded_imgs = self.autoencoder.predict(self.x_test)\n",
    "        return encoded_imgs,decoded_imgs\n",
    "\n",
    "class PlotResuts:\n",
    "    def __init__(self,**kargs):\n",
    "        self.random_test_images = kargs['ri']\n",
    "        self.num_images =kargs['ni']\n",
    "        self.x_test = kargs['x']\n",
    "        self.decoded_imgs = kargs['di']\n",
    "        self.encoded_imgs = kargs['ei']\n",
    "        plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    def plotAll(self):\n",
    "        logger.info('START LOOP OVER TEST IMAGES')\n",
    "        for i, self.image_idx in enumerate(self.random_test_images):\n",
    "            # plot original image\n",
    "            self.plotOne(self.x_test,i + 1,28,28)\n",
    "            # plot encoded\n",
    "            self.plotOne(self.encoded_imgs,num_images + i + 1,8,4)\n",
    "            # plot decoded\n",
    "            self.plotOne(self.decoded_imgs,2*num_images + i + 1,28,28)\n",
    "        plt.show()\n",
    "        \n",
    "    def plotOne(self,x,sec_dim,shape1,shape2):\n",
    "            ax = plt.subplot(3,self.num_images,sec_dim)\n",
    "            plt.imshow(x[self.image_idx].reshape(shape1, shape2))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            \n",
    "def prepareData():\n",
    "    # Loads the training and test data sets (ignoring class labels)\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    # Scales the training and test data to range between 0 and 1.\n",
    "    max_value = float(x_train.max())\n",
    "    x_train = x_train.astype('float32') / max_value\n",
    "    x_test = x_test.astype('float32') / max_value\n",
    "    x_train.shape, x_test.shape\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "    #(x_train.shape, x_test.shape)\n",
    "    input_dim = x_train.shape[1]\n",
    "    return x_train, x_test,input_dim\n",
    "\n",
    "def main():\n",
    "    start = datetime.now()\n",
    "    logger.info('START')\n",
    "    logger.info('PREPARATION OF THE DATE')\n",
    "    x_train,x_test,input_dim = prepareData()\n",
    "    logger.info('SELECTION OT TEST IMAGES')\n",
    "    random_test_images = np.random.randint(x_test.shape[0], size=num_images)\n",
    "    logger.info('CREATION OF THE CLASS FOR THE MODEL')\n",
    "    cnn = CreateNN(id=input_dim,ri=random_test_images,xt=x_train,xte=x_test)\n",
    "    logger.info('AUTOENCODER MODEL')\n",
    "    cnn.createAutoEncoder()\n",
    "    logger.info('ENCODER LAYER')\n",
    "    cnn.layerEncoder()\n",
    "    logger.info('FIT OF THE MODEL')\n",
    "    cnn.modelFit()\n",
    "    logger.info('PREDICTION OVER TEST')\n",
    "    encoded_imgs, decoded_imgs = cnn.modelPredict()\n",
    "    logger.info('PREPARING THE CLASS FOR PRINTING')\n",
    "    pr = PlotResuts(ri=random_test_images,ni=num_images,x=x_test,di=decoded_imgs,ei=encoded_imgs)\n",
    "    logger.info('PLOTTING ALL')\n",
    "    pr.plotAll()\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start)).total_seconds())\n",
    "    logger.info(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a = mnist.load_data()\n",
    "len(a[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a[0][0][0]\n",
    "#a[0][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Exercise #1\n",
    "\n",
    "Give and high level description of the following code.\n",
    "\n",
    "Give a line by line desription of:\n",
    "\n",
    "  - `LoadData`\n",
    "  - `CreateNN`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import logging \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "from keras import backend as K  \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "#Columns selection\n",
    "selectEach = 40\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_2 = 5 # 1st layer number of neurons\n",
    "num_input = 300\n",
    "num_classes = 1\n",
    "\n",
    "metrics = ['accuracy','mse']\n",
    "\n",
    "#Plot parameters definition\n",
    "##############################\n",
    "#IGNORE THIS PART FOR THE EXAM\n",
    "##############################\n",
    "figure(num=None, figsize=(18, 16), dpi=100, facecolor='w', edgecolor='k')\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "###############################\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.filesPath = kargs['fp']\n",
    "        self.fileTarget = kargs['ft']\n",
    "        self.fileFeatures = kargs['ff']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = '%s/%s'%(self.filesPath,fileName)\n",
    "        df = pd.read_csv(fullPath,delimiter=',')\n",
    "        return df\n",
    "\n",
    "    def downsizeFeatures(self,df):\n",
    "        logger.info('MAKING A DIMENSION REDUCTION FOR THE FEATURES')\n",
    "        numFull = df.shape[1]\n",
    "        #Hint: % is the decimal part of a division \n",
    "        columnsSelected = {i for i in range(1,numFull) if i%selectEach==0}\n",
    "        if 0 not in columnsSelected:\n",
    "            columnsSelected.add(0)\n",
    "        df = df.iloc[:,list(columnsSelected)]\n",
    "        return df   \n",
    "        \n",
    "    def createTrain(self):\n",
    "        dfFeaturesFull = self.readFiles(self.fileFeatures)\n",
    "        dfTarget = self.readFiles(self.fileTarget)\n",
    "        logger.warning('ROWS IN THE TWO FILES ARE NOT IN THE SAME ORDER')\n",
    "        dfFeatures = self.downsizeFeatures(dfFeaturesFull)\n",
    "        dfJoin = dfFeatures.set_index('ID').join(dfTarget.set_index('ID'),how = 'inner')\n",
    "        dfJoin.reset_index(inplace=True)\n",
    "        Y = dfJoin.loc[:,'TARGET']\n",
    "        dfJoin.drop(columns=['ID','TARGET'],inplace=True)\n",
    "        print(dfJoin.columns)\n",
    "        return dfJoin, Y \n",
    "    \n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.activationFun = 'relu'\n",
    "        self.X = kargs['x']\n",
    "        self.Y = kargs['y']\n",
    "        self.numFeatures = self.X.shape[1]\n",
    "        self.n_hidden_1 = int(self.numFeatures / 2)\n",
    "        self.modelPath = kargs['mp']\n",
    "        modelName = kargs['mn']\n",
    "        self.modelFile = os.path.join(self.modelPath,modelName)   \n",
    "        \n",
    "    def modelDefinition(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(num_input, input_dim=self.numFeatures,\n",
    "                             activation=self.activationFun))\n",
    "        self.model.add(Dense(self.n_hidden_1,activation=self.activationFun))\n",
    "        self.model.add(Dense(n_hidden_2,activation=self.activationFun))\n",
    "        self.model.add(Dense(num_classes,activation='sigmoid'))\n",
    "              \n",
    "    def modelCompile(self):\n",
    "        sgd = SGD(lr=learning_rate)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=sgd,metrics=metrics)\n",
    "       \n",
    "    def modelTrain(self):\n",
    "        history = self.model.fit(self.X, self.Y, epochs=num_steps,\n",
    "                                 batch_size=batch_size,validation_split=0.05)\n",
    "        self.saveModel()\n",
    "        return history\n",
    "      \n",
    "    def saveModel(self):\n",
    "        logger.info('SAVING MODEL TO %s'%self.modelFile)\n",
    "        #self.model.save('pippo')\n",
    "        #tf.saved_model.save(self.model, \"modelTrained/\")\n",
    "        signature = tf.saved_model.signature_def_utils.predict_signature_def(                                                                        \n",
    "        inputs={'image': self.model.input}, outputs={'scores': self.model.output})                                                                         \n",
    "                                                                                                                                             \n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(self.modelPath)                                                                    \n",
    "        builder.add_meta_graph_and_variables(                                                                                                        \n",
    "        sess=K.get_session(),                                                                                                                    \n",
    "        tags=[tf.saved_model.tag_constants.SERVING],                                                                                             \n",
    "        signature_def_map={                                                                                                                      \n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:                                                                \n",
    "                signature                                                                                                                        \n",
    "                })                                                                                                                                       \n",
    "        builder.save()\n",
    "    \n",
    "class PlotGraphs:\n",
    "    def __init__(self,**kargs):\n",
    "        history= kargs['h']\n",
    "        self.history_dict = history.history\n",
    "        self.metList = []\n",
    "        for cur_key in history.history.keys():\n",
    "            if cur_key.find('val')!=0:\n",
    "                self.metList.append(cur_key)\n",
    "        print(self.metList)\n",
    "        self.lenList = len(self.metList)\n",
    "    \n",
    "    def plotResults(self):\n",
    "        plotPos = 1\n",
    "        for cur_met in self.metList:\n",
    "            cur_values = self.history_dict[cur_met]\n",
    "            cur_val = self.history_dict['val_%s'%cur_met]\n",
    "            epochs = range(1, len(cur_values) + 1)\n",
    "            plt.subplot(self.lenList, 1, plotPos)\n",
    "            plt.plot(epochs, cur_values, 'ro',label = 'train')\n",
    "            plt.plot(epochs, cur_val, 'b+', label='test')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(cur_met)\n",
    "            plt.legend()\n",
    "            #plt.gca().legend(('train','test'))\n",
    "            plotPos += 1\n",
    "        logger.info('SAVING PLOTS TO A FILE')\n",
    "        plt.savefig('historyPlots')\n",
    "\n",
    "def main():\n",
    "    logger.info('START')\n",
    "    start_time = datetime.now()\n",
    "    filesPath = 'data/santAnder'\n",
    "    fileTarget = 'target.csv'\n",
    "    fileFeatures = 'features.csv'\n",
    "    modelPath = 'data/santAnder/modelSaved'\n",
    "    modelName = 'santAnderModel.h5'\n",
    "    logger.info('INSTANTIATION OF LOADDATA')\n",
    "    ld = LoadData(fp=filesPath,ff=fileFeatures,ft=fileTarget)\n",
    "    X_train, Y_train = ld.createTrain()\n",
    "    logger.info('INSTANTIATION OF CREATENN')\n",
    "    cnn = CreateNN(x = X_train,y = Y_train,mp = modelPath, mn = modelName)\n",
    "    cnn.modelDefinition()\n",
    "    cnn.modelCompile()\n",
    "    history = cnn.modelTrain()\n",
    "    logger.info('INITIALIZATION OF PLOTGRAPH')\n",
    "    pg = PlotGraphs(h=history)\n",
    "    pg.plotResults()\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info('END')\n",
    "    return X_train, Y_train, history\n",
    "X_train,Y_train, history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "#logger.debug('Pippo')\n",
    "#logger.info('PLuto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## End Exercise #2 \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give an *high level* description of what the code does\n",
    "  - Give a *line by line* description of the methodos `makeJoin` and `preProcess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "dataFolder = 'data'\n",
    "chifData = 'chifdata'\n",
    "prospFile = 'all_prospects_france.csv'\n",
    "\n",
    "\n",
    "class MakeClusters:\n",
    "    def __init__(self,**kargs):\n",
    "        self.outFile = kargs['fo']\n",
    "        self.numClus = kargs['nk']\n",
    "        self.numEmp = kargs['ne']\n",
    "        self.siren = kargs['si']\n",
    "        self.to = kargs['to']\n",
    "\n",
    "\n",
    "\n",
    "    def makeClusters(self,df):\n",
    "        logger.info('Defining KMeans with %i clusters',self.numClus)\n",
    "        kmeans = KMeans(n_clusters=self.numClus)\n",
    "        kmeans.fit(df)\n",
    "        labels = kmeans.predict(df)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        logger.info('Adding labels to the dataframes')\n",
    "        df['labels'] = labels\n",
    "        df['Rsultat 1'] = self.to\n",
    "        df['Siren'] = self.siren\n",
    "        return df\n",
    "\n",
    "    def writeOutFile(self,df):\n",
    "        logger.info('Writing out file to %s',self.outFile)\n",
    "        df.to_csv(self.outFile,index=False)\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, **kargs):\n",
    "        self.inFile = kargs['fi']\n",
    "        self.inDir = kargs['di']\n",
    "        self.numEmp = kargs['ne']\n",
    "\n",
    "    def readOne(self):\n",
    "        logger.info('Reading data from %s',self.inFile)\n",
    "        self.dfPros = pd.read_csv(self.inFile,sep=',')\n",
    "        print('columns for prof',self.dfPros.columns,self.dfPros.shape,self.dfPros.dtypes)\n",
    "\n",
    "    def readMultiple(self):\n",
    "        self.dfInfo = pd.DataFrame()\n",
    "        for filename in os.listdir(self.inDir):\n",
    "            fi = \"%s/%s\"%(self.inDir,filename)\n",
    "            logger.info('Readin file %s',fi)\n",
    "            curdf = pd.read_csv(fi,sep=';',usecols=['Siren','Rsultat 1'])\n",
    "            self.dfInfo = self.dfInfo.append(curdf)\n",
    "        print('columns for info',self.dfInfo.columns,self.dfInfo.shape,self.dfInfo.dtypes)\n",
    "\n",
    "    def makeJoin(self):\n",
    "        logger.info('Making the join')\n",
    "        self.dfJoin = self.dfPros.set_index('SIREN').join(self.dfInfo.set_index('Siren'),how='inner')\n",
    "        self.dfJoin.reset_index(inplace=True)\n",
    "        print('shape after join:',self.dfJoin.shape,self.dfJoin.columns)\n",
    "        return self.dfJoin\n",
    "\n",
    "    def preProcess(self,df):\n",
    "        df.dropna(inplace=True)\n",
    "        logger.info('Removing rows with more that %i employes',self.numEmp)\n",
    "        df = df.loc[df['eff_category']<=self.numEmp]\n",
    "        logger.info('SCALING VARIABILES')\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        np_scaled = min_max_scaler.fit_transform(pd.DataFrame(df.loc[:,'Rsultat 1']))\n",
    "        df['to_norm'] = np_scaled\n",
    "        to = df[['Rsultat 1']]\n",
    "        siren = df[['index']]\n",
    "        df.drop(['Rsultat 1','index'],axis=1,inplace=True)\n",
    "        return df, to, siren\n",
    "\n",
    "def main():\n",
    "    start=datetime.now()\n",
    "    logger.info(\"START\")\n",
    "    inFile = \"%s/%s\"%(dataFolder,prospFile)\n",
    "    inDir = \"%s/%s\"%(dataFolder,chifData)\n",
    "    numClus = 5\n",
    "    outFile = 'testClustering.csv'\n",
    "    numEmp = 10\n",
    "    logger.info(\"RUNNING WITH INFILE %s, OUTFILE %s , NUM EMPLOIES %d AND NUM OF CLUSTER %d\",inFile,outFile,numEmp,numClus)\n",
    "    logger.info('INSTANTIATE PREPAREDATA')\n",
    "    pdt = PrepareData(fi=inFile,di=inDir,ne=numEmp)\n",
    "    pdt.readOne()\n",
    "    pdt.readMultiple()\n",
    "    df = pdt.makeJoin()\n",
    "    df,to,siren = pdt.preProcess(df)\n",
    "    logger.info('INSTANTIATE MAKECLUSTER')\n",
    "    mc = MakeClusters(fo=outFile,nk=numClus,ne=numEmp,si=siren,to=to)\n",
    "    df = mc.makeClusters(df)\n",
    "    mc.writeOutFile(df)\n",
    "    logger.info('DONE IN %s',str(datetime.now()-start))\n",
    "    logger.info('END')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "selNum = 20\n",
    "a = {i for i in range(1,100) if i%selNum==0}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selNum = 20\n",
    "b = set()\n",
    "for i in range(1,100):\n",
    "    if i%selNum == 0:\n",
    "        b.add(i)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
